# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16CJlS3OL3zyco6m1TrHm5m5eNxPh8ZpE
"""

!pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

def scrape_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    paragraphs = soup.find_all('p')
    text = ' '.join([para.get_text() for para in paragraphs])
    return text

# Example usage
url = 'https://www.psychologytoday.com/intl/blog/full-living/202407/silliness-is-a-gateway-drug'
scraped_text = scrape_text_from_url(url)
print(scraped_text)

# Save the scraped text to a file
# file_path = 'scraped_text.txt'
# with open(file_path, 'w', encoding='utf-8') as file:
#     file.write(scraped_text)
# Step 5: Save the scraped text to a file
file_path = 'scraped_text.txt'
with open(file_path, 'w', encoding='utf-8') as file:
    file.write(scraped_text)

print(f"Scraped text has been saved to {file_path}")

# Step 6: Download the file to your local machine
from google.colab import files

files.download('scraped_text.txt')

import requests
from bs4 import BeautifulSoup

def scrape_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    paragraphs = soup.find_all('p')
    text = ' '.join([para.get_text() for para in paragraphs])
    return text

# List of URLs to scrape
urls = [
    'https://mhanational.org/lessons-MHA-screening',
    'https://www.mhanational.org/issues/state-mental-health-america',
    'https://mhanational.org/issues/action-alerts',
    'https://www.webmd.com/depression/alternative-therapies-depression',
    'https://www.healthline.com/health/depression/how-to-fight-depression',
    'https://www.healthline.com/health/depression',
    'https://www.healthline.com/health/adhd',
    'https://www.hopkinsmedicine.org/health/conditions-and-diseases/major-depression',
    'https://my.clevelandclinic.org/health/diseases/9290-depression',
    'https://www.medicalnewstoday.com/articles/320502',
    'https://www.helpguide.org/articles/depression/coping-with-depression.htm',
    'https://www.who.int/news-room/fact-sheets/detail/depression',
    'https://www.mayoclinic.org/diseases-conditions/depression/diagnosis-treatment/drc-20356013',
    'https://thestoryexchange.org/6-ways-to-manage-stress-for-mental-health-awareness-month/?gclid=CjwKCAjw7c2pBhAZEiwA88pOF-KN52KywZXCp8lwUNidiHZ8hk2dXRrCn8dwmWjj2sWRtXq3sz9rzRoCXvIQAvD_BwE&gad_source=1&gclid=CjwKCAjw1920BhA3EiwAJT3lSU7DIyxNMpGhh8lD6TiMuszaWNSpolttcYibxeU8405HDeUVImg1fBoC5EgQAvD_BwE',
    'https://www.nhs.uk/mental-health/self-help/tips-and-support/cope-with-depression/',
    'https://www.webmd.com/depression/alternative-therapies-depression',
    'https://kidshealth.org/en/teens/depression-tips.html',
    # Add more URLs as needed
]

# Dictionary to store the scraped text for each URL
scraped_texts = {}

# Iterate through the URLs and scrape the text
for url in urls:
    try:
        text = scrape_text_from_url(url)
        scraped_texts[url] = text
        print(f"Successfully scraped: {url}")
    except Exception as e:
        print(f"Failed to scrape: {url} with error {e}")

# Save the scraped texts to a single file
file_path = 'scraped_texts.txt'
with open(file_path, 'w', encoding='utf-8') as file:
    for url, text in scraped_texts.items():
        file.write(f"URL: {url}\n{text}\n\n")

print(f"Scraped texts have been saved to {file_path}")

# Optional: Save each URL's text in separate files
# for url, text in scraped_texts.items():
#     filename = url.split('/')[-1] + '.txt'
#     with open(filename, 'w', encoding='utf-8') as file:
#         file.write(text)
#     print(f"Saved {filename}")

# Download the file to your local machine
from google.colab import files
files.download(file_path)



import requests
from bs4 import BeautifulSoup

def scrape_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    paragraphs = soup.find_all('p')
    text = ' '.join([para.get_text() for para in paragraphs])
    return text

# List of URLs to scrape
urls = [
    'https://www.nimh.nih.gov/health/topics/caring-for-your-mental-health',
    'http://www.bcmhsus.ca/about/news-stories/stories/10-tips-to-boost-your-mental-health',
    'https://www.mentalhealth.org.uk/explore-mental-health/publications/our-best-mental-health-tips',
    'https://mhanational.org/31-tips-boost-your-mental-health',
    'https://www.mind.org.uk/information-support/tips-for-everyday-living/wellbeing/',
    'https://www.nhs.uk/mental-health/self-help/guides-tools-and-activities/five-steps-to-mental-wellbeing/',
    'https://medlineplus.gov/howtoimprovementalhealth.html',
    'https://www.helpguide.org/articles/mental-health/building-better-mental-health.htm',
    'https://www.who.int/westernpacific/about/how-we-work/pacific-support/news/detail/07-10-2021-6-ways-to-take-care-of-your-mental-health-and-well-being-this-world-mental-health-day',
    'https://bbrfoundation.org/blog/everyday-mental-health-tips'
    # Add more URLs as needed
]

# Dictionary to store the scraped text for each URL
scraped_texts = {}

# Iterate through the URLs and scrape the text
for url in urls:
    try:
        text = scrape_text_from_url(url)
        scraped_texts[url] = text
        print(f"Successfully scraped: {url}")
    except Exception as e:
        print(f"Failed to scrape: {url} with error {e}")

# Save the scraped texts to a single file
file_path = 'scraped_texts.txt'
with open(file_path, 'w', encoding='utf-8') as file:
    for url, text in scraped_texts.items():
        file.write(f"URL: {url}\n{text}\n\n")

print(f"Scraped texts have been saved to {file_path}")

# Optional: Save each URL's text in separate files
# for url, text in scraped_texts.items():
#     filename = url.split('/')[-1] + '.txt'
#     with open(filename, 'w', encoding='utf-8') as file:
#         file.write(text)
#     print(f"Saved {filename}")

# Download the file to your local machine
from google.colab import files
files.download(file_path)

# Training Models

!pip install transformers torch PyPDF2 ipywidgets

import os
from PyPDF2 import PdfReader
import glob

from google.colab import drive
drive.mount('/content/drive')

def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def read_pdf_file(file_path):
    text = ""
    with open(file_path, 'rb') as file:
        reader = PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Specify the directory in Google Drive
data_dir = '/content/drive/MyDrive/Mental Illness'

# Collect all text data from .txt and .pdf files
text_data = ""

# Read all .txt files
for txt_file in glob.glob(os.path.join(data_dir, '*.txt')):
    text_data += read_text_file(txt_file) + "\n"

# Read all .pdf files
for pdf_file in glob.glob(os.path.join(data_dir, '*.pdf')):
    text_data += read_pdf_file(pdf_file) + "\n"

# Save the combined text data to a single file
combined_file_path = 'combined_text_data.txt'
with open(combined_file_path, 'w', encoding='utf-8') as file:
    file.write(text_data)

print(f"Combined text data saved to {combined_file_path}")

# To Download file

from google.colab import files
files.download('combined_text_data.txt')

!pip install transformers datasets
!pip install accelerate

from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import Dataset

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Set the padding token
tokenizer.pad_token = tokenizer.eos_token

model = GPT2LMHeadModel.from_pretrained(model_name)

# Load dataset
def load_dataset_from_text(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    data = {'text': [text]}
    dataset = Dataset.from_dict(data)
    return dataset

dataset = load_dataset_from_text('combined_text_data.txt')

# Tokenize dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-mental-health",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=1,
    save_steps=10_000,
    save_total_limit=2,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset,
)

# Train the model
trainer.train()
trainer.save_model("./gpt2-mental-health")
tokenizer.save_pretrained("./gpt2-mental-health")

import os
from PyPDF2 import PdfReader
import glob

# Example questions for prompts
questions = [
    "What are the common symptoms of depression?",
    "How can I manage my anxiety on a daily basis?",
    "What are some effective treatments for depression?",
    "How does cognitive-behavioral therapy help with anxiety?",
    "What are the warning signs of a panic attack?",
    "How can I support a friend who is dealing with depression?",
    "What lifestyle changes can help reduce anxiety?",
    "What are the potential side effects of antidepressant medications?",
    "How can mindfulness meditation help with anxiety?",
    "What is the difference between anxiety and depression?",
    "How can exercise impact mental health?",
    "What are some coping strategies for dealing with stress?",
    "How does diet affect mental health?",
    "What should I do if I feel overwhelmed by anxiety?",
    "How can I improve my sleep when dealing with depression?",
    "What are the benefits of joining a support group for mental health?",
    "How can I recognize the signs of burnout?",
    "What is the role of a therapist in managing mental health?",
    "How can journaling help with anxiety and depression?",
    "What are some natural remedies for anxiety?"
]

# Function to read text files
def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# Function to read PDF files
def read_pdf_file(file_path):
    text = ""
    with open(file_path, 'rb') as file:
        reader = PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Specify the directory in Google Drive
data_dir = '/content/drive/MyDrive/Mental Illness'

# Collect all text data from .txt and .pdf files
text_data = ""

# Read all .txt files
for txt_file in glob.glob(os.path.join(data_dir, '*.txt')):
    text_data += read_text_file(txt_file) + "\n"

# Read all .pdf files
for pdf_file in glob.glob(os.path.join(data_dir, '*.pdf')):
    text_data += read_pdf_file(pdf_file) + "\n"

# Add prompt to each line of text data and example questions
prompted_text_data = ""
for line in text_data.split('\n'):
    if line.strip():  # Only add prompt to non-empty lines
        prompted_text_data += f"Answer the following question: {line}\n"

for question in questions:
    prompted_text_data += f"Answer the following question: {question}\n"

# Save the combined text data to a single file
combined_file_path = 'combined_text_data_with_prompts.txt'
with open(combined_file_path, 'w', encoding='utf-8') as file:
    file.write(prompted_text_data)

print(f"Combined text data with prompts saved to {combined_file_path}")

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Load the fine-tuned model and tokenizer
model = GPT2LMHeadModel.from_pretrained("./gpt2-mental-health")
tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-mental-health")

# Function to generate a response from the model
def generate_response(prompt):
    inputs = tokenizer.encode(f"Answer the following question: {prompt}", return_tensors='pt')
    outputs = model.generate(inputs, max_length=150, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Create an input field in Google Colab
import ipywidgets as widgets
from IPython.display import display

input_box = widgets.Text(
    value='',
    placeholder='Type your question here',
    description='Input:',
    disabled=False
)

output_box = widgets.Textarea(
    value='',
    placeholder='Output will be displayed here',
    description='Output:',
    disabled=False
)

def on_submit(change):
    prompt = input_box.value
    response = generate_response(prompt)
    output_box.value = response

input_box.on_submit(on_submit)

display(input_box, output_box)

!pip install pyngrok

from pyngrok import ngrok

# Start ngrok tunnel
public_url = ngrok.connect(port="5000")
print(f" * ngrok tunnel available at: {public_url}")

# If other things doesn't work

# then run below code

!pip install flask-ngrok
!pip install transformers datasets accelerate pyngrok

from pyngrok import ngrok

# Replace <YOUR_NGROK_AUTHTOKEN> with your actual ngrok authtoken
ngrok.set_auth_token("<YOUR_NGROK_AUTHTOKEN>")

from flask import Flask, request, jsonify
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)  # Start ngrok when the app is run

# Load the fine-tuned model and tokenizer
model = GPT2LMHeadModel.from_pretrained("./gpt2-mental-health")
tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-mental-health")

# Function to generate a response from the model
def generate_response(prompt):
    inputs = tokenizer.encode(f"Answer the following question: {prompt}", return_tensors='pt')
    outputs = model.generate(inputs, max_length=150, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

@app.route('/api/generate', methods=['POST'])
def generate():
    data = request.json
    prompt = data.get('prompt')
    response = generate_response(prompt)
    return jsonify({'response': response})

if __name__ == '__main__':
    app.run()

from pyngrok import ngrok

# Start ngrok tunnel
public_url = ngrok.connect(port="5000")
print(f" * ngrok tunnel available at: {public_url}")

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mental Health Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
        }
        .container {
            text-align: center;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        input[type="text"] {
            width: 80%;
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
            border: 1px solid #ccc;
        }
        button {
            padding: 10px 20px;
            border-radius: 5px;
            border: none;
            background-color: #007bff;
            color: white;
            cursor: pointer;
        }
        button:hover {
            background-color: #0056b3;
        }
        .response {
            margin-top: 20px;
            padding: 10px;
            border-radius: 5px;
            background-color: #e9ecef;
            border: 1px solid #ced4da;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Mental Health Assistant</h1>
        <input type="text" id="question" placeholder="Type your question here">
        <button onclick="getResponse()">Get Response</button>
        <div id="response" class="response"></div>
    </div>
    <script src="script.js"></script>
</body>
</html>

async function getResponse() {
    const question = document.getElementById('question').value;
    const responseDiv = document.getElementById('response');

    const response = await fetch('<YOUR_NGROK_URL>/api/generate', {  // Replace <YOUR_NGROK_URL> with the actual ngrok URL
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({ prompt: question })
    });

    const data = await response.json();
    responseDiv.textContent = data.response;
}

const response = await fetch('http://<ngrok-id>.ngrok.io/api/generate', {  // Replace <ngrok-id> with your actual ngrok ID
    method: 'POST',
    headers: {
        'Content-Type': 'application/json'
    },
    body: JSON.stringify({ prompt: question })
});